{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Transformers Library (Colab Recommended) ğŸ¤—\n",
    "\n",
    "\n",
    "To recap, HuggingFace is an AI company that has blown up in the last few years, especially in the realm of Natural Language Processing (NLP). \n",
    "\n",
    "In particular, the Transformers library has revolutionized the way people  work with large-scale transformer models. The goal of this challenge is to introduce you to these models for the first time and show how easy they can be to work with. \n",
    "\n",
    "### Why you should love HuggingFace:\n",
    "\n",
    "#### Pre-trained Models ğŸ“š: \n",
    "\n",
    "One of the best features of the Transformers library is its huge repo of pre-trained models. Whether you're looking to employ BERT, GPT-2, T5, RoBERTa, or any of the other transformer architectures, chances are you'll find a version that suits your needs in their model hub.\n",
    "\n",
    "#### It's super easy ğŸ‘: \n",
    "\n",
    "The library is designed to be user-friendly. Loading a model and its corresponding tokenizer can be done in just a couple of lines of code. This simplicity extends to fine-tuning as well, allowing you to adapt these powerful models to a wide range of tasks. The `pipelines` library we'll be using lets you go from model selection to getting results in just a few lines.\n",
    "\n",
    "#### Tokenizer  ğŸ”„ and Datasets ğŸ“Š Library: \n",
    "\n",
    "Alongside the Transformers library, HuggingFace also offers the Tokenizers and Datasets libraries. While the first provides efficient and easy-to-use tokenization methods, the second offers a whole bunch of datasets, meaning you have all the tools and data you need in one ecosystem.\n",
    "\n",
    "#### Community-Driven ğŸŒ: \n",
    "The HuggingFace community is very active and any community member (you included) can upload their own models and datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you are working in Colab__ you'll need to install the appropriate libraries in your Colab environment (you will have them locally if you followed the setup instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: torch in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (2.6.0+cpu.cxx11.abi)\n",
      "Requirement already satisfied: pytesseract in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (0.3.13)\n",
      "Requirement already satisfied: filelock in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: setuptools in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from torch) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from pytesseract) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "# Install the transformers library from HuggingFace\n",
    "!pip install transformers torch pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: sacremoses in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (0.1.1)\n",
      "Requirement already satisfied: regex in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from sacremoses) (2024.11.6)\n",
      "Requirement already satisfied: click in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from sacremoses) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from sacremoses) (1.4.2)\n",
      "Requirement already satisfied: tqdm in /home/julio/.pyenv/versions/3.12.9/envs/lewagon/lib/python3.12/site-packages (from sacremoses) (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "# You'll also need some extra tools that some of these models use under the hood\n",
    "! pip install sentencepiece sacremoses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over the course of this notebook, you'll be using Pipelines to download and easily use some very powerful models. Bear in mind that some of these models are quite large (up to 500Mb so make sure you have some disk space free on your machine or run this notebook in a Colab with faster download speeds!). \n",
    "\n",
    "We are going to be using pre-built models and the best resource for implementing them will be using the [Pipelines documentation](https://huggingface.co/docs/transformers/main_classes/pipelines). If you ever want to delete the models locally after use, you can find them here in your root directory at:\n",
    "\n",
    "`/.cache/huggingface/hub`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Sentiment : ğŸ˜€ /  ğŸ˜• / ğŸ˜  / ğŸ˜Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in mind, instantiate a pipeline for sentiment analysis __without__ specifying a model and try testing out that model with the sentence \"Transformers are awesome!\" Feel free to try some other sentences, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 'POSITIVE', 'score': 0.9998667240142822}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline('sentiment-analysis')\n",
    "result = pipe(\"Transformers are awesome!\")\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nuanced Sentiment ğŸ¤”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFace will default to using `distilbert-base-uncased-finetuned-sst-2-english` if we don't specify a model. This model will work fine on a lot of basic use cases, but - because it's been trained on a fairly limited corpus of text: \n",
    "\n",
    "`The Stanford Sentiment Treebank is a corpus with fully labeled parse trees that allows for a complete analysis of the compositional effects of sentiment in language. The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.`\n",
    "\n",
    "It's fairly obvious that a model trained on this will likely perform poorly on sentences that include modern language: e.g. \"These beats are sick!\". Try running these sentences through your pipeline now and you should get negative scores even though they are expressing quite positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'NEGATIVE', 'score': 0.9997040629386902}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pipe(\"These beats are sick!\")\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to the list of HuggingFace models to see if you can find a model that will specialize on Twitter sentiment (looking for `\"twitter-roberta-base-sentiment-latest\"` might be a good place to start) - hopefully that should be a bit more up to date with all this new lingo! Now create a second pipeline, this time __specifying__ that model that we want to use (use `model=`) and see how our performance instantly improves now we're using a fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 'positive', 'score': 0.788419246673584}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_2 = pipeline('sentiment-analysis',model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\n",
    "result_2 = pipe_2(\"These beats are sick!\")\n",
    "result_2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a much more accurate interpretation of the sentiment we're trying to express."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment in other languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While even our first pipeline will actually perform surprisingly well on simple sentences in other languages (e.g. \"C' est bon\" or \"Esta bueno\"), it breaks down when handling more sophisticated ideas in those languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example review for the Jurassic World Dominion movie ğŸ˜¬: \n",
    "\n",
    "\"This was frankly a spectacular failure from start to finish, with  remarkably uninspired performances from some very well-paid actors who acted with all the passion of a wet biscuit\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranlated into Korean it reads as this: \"ì´ê²ƒì€ ì†”ì§íˆ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì—„ì²­ë‚œ ì‹¤íŒ¨ì˜€ìœ¼ë©° ì –ì€ ë¹„ìŠ¤í‚·ì˜ ëª¨ë“  ì—´ì •ìœ¼ë¡œ ì—°ê¸°í•œ ì¼ë¶€ ë§¤ìš° ë³´ìˆ˜ê°€ ì¢‹ì€ ë°°ìš°ë“¤ì˜ í˜„ì €í•˜ê²Œ ì˜ê°ì„ ë°›ì§€ ëª»í•œ ì—°ê¸°ë¡œ ëë‚¬ìŠµë‹ˆë‹¤.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the Korean text through either your Twitter model; you should see they won't pick up on how bad the review is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'neutral', 'score': 0.7584188580513}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_3 = pipe_2(\"ì´ê²ƒì€ ì†”ì§íˆ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì—„ì²­ë‚œ ì‹¤íŒ¨ì˜€ìœ¼ë©° ì –ì€ ë¹„ìŠ¤í‚·ì˜ ëª¨ë“  ì—´ì •ìœ¼ë¡œ ì—°ê¸°í•œ ì¼ë¶€ ë§¤ìš° ë³´ìˆ˜ê°€ ì¢‹ì€ ë°°ìš°ë“¤ì˜ í˜„ì €í•˜ê²Œ ì˜ê°ì„ ë°›ì§€ ëª»í•œ ì—°ê¸°ë¡œ ëë‚¬ìŠµë‹ˆë‹¤.\")\n",
    "result_3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now see if you can find a model that might perform better in the HuggingFace library and use it. Try using `\"matthewburke/korean_sentiment\"` in a `text-classification` pipeline and see if your results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'label': 'LABEL_0', 'score': 0.9615505337715149}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_3 = pipeline('text-classification',model=\"matthewburke/korean_sentiment\")\n",
    "result_3 = pipe_3(\"ì´ê²ƒì€ ì†”ì§íˆ ì²˜ìŒë¶€í„° ëê¹Œì§€ ì—„ì²­ë‚œ ì‹¤íŒ¨ì˜€ìœ¼ë©° ì –ì€ ë¹„ìŠ¤í‚·ì˜ ëª¨ë“  ì—´ì •ìœ¼ë¡œ ì—°ê¸°í•œ ì¼ë¶€ ë§¤ìš° ë³´ìˆ˜ê°€ ì¢‹ì€ ë°°ìš°ë“¤ì˜ í˜„ì €í•˜ê²Œ ì˜ê°ì„ ë°›ì§€ ëª»í•œ ì—°ê¸°ë¡œ ëë‚¬ìŠµë‹ˆë‹¤.\")\n",
    "result_3[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation âœï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's stick with our language theme and see if we can find a model that can handle the tasks of translating some sentences for us. The `opus-mt` project from the University of Helsinki is incredibly active on HuggingFace, creating and maintaining models designed to democratize the translation process for many different global languages. Try implementing the `\"Helsinki-NLP/opus-mt-<source-language>-<destination-language>\"` to see if you can translate between two langauges (e.g. English to Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbb27578f13406ba832ae2d36469945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb9b5e82c8914196905dfec624168902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9a1cb919c544b3e9df74bb63b3c853b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf49b9cc62e4d6b964d215cd8c236ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519085f7bc484fc0922fe5cdba1275c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dca52bf6da840d1928774840dea999a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a23648be2f441dc8f27ee21b50875f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb6d2d64d074eea93881a00fc4a8d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'translation_text': 'Hola mundo'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_4 = pipeline('translation_en_to_es',model=\"Helsinki-NLP/opus-mt-en-es\")\n",
    "result_4 = pipe_4('hello world')\n",
    "result_4[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another really useful NLP task is summarizing a large amount of information into a very small amount of words. BART is a model that performs well on tasks like summarization; it contains a combination of two models you've already seen briefly in the lecture - the BERT model and autogressive style GPT model - check out this [link](https://www.projectpro.io/article/transformers-bart-model-explained/553) for some more information on it. \n",
    "\n",
    "Since BART models can be quite large, try to find the `distilbart-xsum-12-6` model on HuggingFace which is one of the smallest distillations available (we'll talk more about distillations later!). Integrate that model into a `\"summarization\"` pipeline, then take some text (e.g. perhaps by copy-pasting [a BBC article](https://www.bbc.com/news/topics/cx2pk70323et)) and summarize it with your pipeline!\n",
    "\n",
    "N.B. You need to be careful about context windows - here, you may run into an issue with your input being too long for the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Olympic double gold medallist Nicola Adams said dropping in on a children's club session \"brings back memories from when I started boxing\".\n",
    "\n",
    "Adams, 42, ran a masterclass at Ipswich Boxing Club in Suffolk on Wednesday, where her uncle Rob Ottley is one of the coaches.\n",
    "\n",
    "\"It's been a great night - she's got stuck in with everyone, doing pads. The kids loved it,\" said Mr Ottley.\n",
    "\n",
    "Adams, who became Olympic champion in 2012 and retired in 2019, said seeing the young boxers \"makes me relive everything  I almost wanted to get back in the ring\".\n",
    "\n",
    "Jamie Niblock/BBC A blonde haired boy wearing black boxing gloves prepares to punch the punchbag while Nicola stands next to it. Jamie Niblock/BBC\n",
    "Adams said \"a boxing gym is such a safe space\", especially for young people\n",
    "Adams took up boxing as a teenager in West Yorkshire, and said the sport offered young people a sense of purpose.\n",
    "\n",
    "\"It helped me out a lot when I was younger, growing up on a council estate in Leeds,\" she said.\n",
    "\n",
    "\"It gave me focus and drive and something to dedicate my life to.\"\n",
    "\n",
    "After retaining her flyweight title in Rio in 2016 and turning pro in 2017, Adams stepped down from the sport over fears she could lose her sight.\n",
    "\n",
    "But she hasn't lost her passion for the ring.\n",
    "\n",
    "\"I'm here for the love of boxing, and I like to help out the kids,\" she said.\n",
    "\n",
    "\"I think it's always inspiring when you get to see someone in the flesh who's achieved so much in boxing. It gives you so much more motivation,\" she said.\n",
    "\n",
    "Jamie Niblock/BBC Eilish Tierney smiles at the camera. She wears a black adidas vest and has tattoos on both arms and shoulders - the visible ones are of Cupid flying with an arrow. Her brown hair is tied back and she has a silver nose ring. Jamie Niblock/BBC\n",
    "Eilish Tierney said it was \"incredible\" to have someone like Nicola \"pioneer the way for us\"\n",
    "As well as becoming the first ever female Olympic boxing gold medallist in 2012, Adams was also the first open member of the LGBT community to become champion.\n",
    "\n",
    "Known for her appearance on Strictly Come Dancing alongside Katya Jones, she is the new host of the BBC's LGBT Sport Podcast.\n",
    "\n",
    "Eilish Tierney, the first professional female boxer in Ipswich, was also at the club and trained with Adams.\n",
    "\n",
    "\"To have someone of her calibre give me advice is mental,\" she said.\n",
    "\n",
    "\"She opened the door for so many young females to consider being pro, let alone boxing.\n",
    "\n",
    "\"It's absolutely incredible what she's doing for women, gay women, black women. To have met such an amazing woman, let alone a boxer  it's awesome.\"\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'It takes a whole whole new set of rules and regulations for the United States and is expected to first rule in the USA by the end of the year.'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_4 = pipeline('summarization',model=\"aware-ai/distilbart-xsum-12-6-squadv2\")\n",
    "result_4 = pipe_4(text, max_length=100, min_length=30, do_sample=False)\n",
    "result_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further: Question Answering ğŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to go further than just a summary? Perhaps asking questions about a specific dataset in an intuitive way? There's a model for that, too! Enter the (reasonably small) `roberta-base-squad2` - a model trained on question-answer pairs that can answer a `question` about a provided `context` (a body of text you will provide). Check the docs [here](https://huggingface.co/deepset/roberta-base-squad2?context=The+Amazon+rainforest+%28Portuguese%3A+Floresta+Amaz%C3%B4nica+or+Amaz%C3%B4nia%3B+Spanish%3A+Selva+Amaz%C3%B3nica%2C+Amazon%C3%ADa+or+usually+Amazonia%3B+French%3A+For%C3%AAt+amazonienne%3B+Dutch%3A+Amazoneregenwoud%29%2C+also+known+in+English+as+Amazonia+or+the+Amazon+Jungle%2C+is+a+moist+broadleaf+forest+that+covers+most+of+the+Amazon+basin+of+South+America.+This+basin+encompasses+7%2C000%2C000+square+kilometres+%282%2C700%2C000+sq+mi%29%2C+of+which+5%2C500%2C000+square+kilometres+%282%2C100%2C000+sq+mi%29+are+covered+by+the+rainforest.+This+region+includes+territory+belonging+to+nine+nations.+The+majority+of+the+forest+is+contained+within+Brazil%2C+with+60%25+of+the+rainforest%2C+followed+by+Peru+with+13%25%2C+Colombia+with+10%25%2C+and+with+minor+amounts+in+Venezuela%2C+Ecuador%2C+Bolivia%2C+Guyana%2C+Suriname+and+French+Guiana.+States+or+departments+in+four+nations+contain+%22Amazonas%22+in+their+names.+The+Amazon+represents+over+half+of+the+planet%27s+remaining+rainforests%2C+and+comprises+the+largest+and+most+biodiverse+tract+of+tropical+rainforest+in+the+world%2C+with+an+estimated+390+billion+individual+trees+divided+into+16%2C000+species.&question=How+many+species+are+in+the+Amazon%3F)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You know the drill: Create a `\"question-answering\"` pipeline with the `roberta-base-squad2` model, then try putting the `article` you picked before as your context and try asking a `question` about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b04314939094b32a6c574b19fb29ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26c4c8294cd4801bbd513b17773ce6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/496M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14cbf7ea394246ceac25604903e41722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/79.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9781b480ea234ef08d6c05ed5df3fef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f577e8d728b4d67aeb2d6dfda73ccfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5393286ff8604f01b3e71f36a4742b5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': 0.6950756311416626, 'start': 149, 'end': 151, 'answer': '42'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_5 = pipeline('question-answering',model=\"deepset/roberta-base-squad2\")\n",
    "result_5 = pipe_5(\n",
    "    question=\"How old is Adam?\",\n",
    "    context=text,\n",
    ")\n",
    "result_5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speech to text ğŸ¤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best models for converting speech to text was made is the open source Whisper model made by OpenAI (creator of ChatGPT etc.) Take a look at the diagram of the model architecture - it should now look quite similar to those you've already seen today:\n",
    "\n",
    "\n",
    "<img src = https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/whipser.png width = 450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command to download this audio sample and install some additional required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for julio: "
     ]
    }
   ],
   "source": [
    "# Uncomment line below for Windows/ Linux/ Colab\n",
    "!sudo apt install ffmpeg\n",
    "\n",
    "# Uncomment line below for Mac users\n",
    "!HOMEBREW_NO_AUTO_UPDATE=1 brew install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be93d2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!curl https://wagon-public-datasets.s3.amazonaws.com/deep_learning_datasets/harvard.wav > data/harvard.wav"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can listen to the clip by using the by importing `IPython` and loading the audio file (see the Algebra day recap for an example of how this is done!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the smallest Whisper model version on HuggingFace (`whisper-tiny`) and use it to transcribe the audio. Try it on some other `.wav` files if you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus: Let's get multimodal ğŸ˜: Visual Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use question-answering style models on images if we'd like. Many of these models will use chains under the hood that will extract text from an image then pass it through to a language model. In order to use the following model you will need to make sure you `pip install Pillow pytesseract` which are two libraries that will help us to extract text from our images. \n",
    "\n",
    "Once that's done, we're going to create a `\"document-question-answering\"` pipeline - we'll need a model for it, so search for the `layoutlm-invoices` model on HuggingFace. Then try to ask questions about this [`receipt.webp`](https://wagon-public-datasets.s3.amazonaws.com/data-science-images/lectures/Transformers/receipt.webp) (you download the image to your data folder or you can pass the url directly into your model when you call it). Try asking how much the eggs cost, what sales tax was and what the total was. Feel free to try it on some of your own images!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to run, you'll need some dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Mac, uncomment:\n",
    "# !brew install tesseract\n",
    "\n",
    "# For Linux or Colab etc. uncomment these:\n",
    "# !sudo apt install tesseract-ocr\n",
    "# !sudo apt install libtesseract-dev\n",
    "\n",
    "# Then restart your kernel and give it a try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats ğŸ‰ You've just seen how simple it can be to start working with some advanced Transformer-based models and we've only just scratched the surface.\n",
    "\n",
    "There are so many models you can explore in the HuggingFace library for all kinds of different tasks. Your imagination is literally the limit (well - your compute power can also be a limit somtimes ğŸ˜…). To take these models even further for custom usage, we're going to tackle fine-tuning next. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸âš ï¸âš ï¸ If you have been running these models locally, don't forget to clean up your `/.cache/huggingface/hub` if you're limited on space or you'll have a lot of unwanted models hanging around in your cache ğŸ§¹ âš ï¸âš ï¸âš ï¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
